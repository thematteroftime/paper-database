import os
from pathlib import Path
from openai import OpenAI
import faiss
import numpy as np
import sqlite3
import hashlib
import json
import portalocker
import fitz  # PyMuPDFï¼Œç”¨äºä» PDF ä¸­æå–å›¾åƒ
import base64

# 1. åˆå§‹åŒ–å®¢æˆ·ç«¯ä¸é…ç½®
client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY") or "sk-fd7afdef962a46d39784e8b0b8133974",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

PROJECT_ROOT = Path(__file__).resolve().parent


class ComplexPlasmaRAG:
    def __init__(self, db_path="plasma_knowledge.db",
                 paper_idx_path="faiss_papers.index",
                 force_idx_path="faiss_forces.index",
                 api_key=None):
        # å¦‚æœä¼ å…¥äº† api_keyï¼Œå°±æ›´æ–°å…¨å±€çš„ clientï¼ˆæˆ–è€…æ–°å»ºä¸€ä¸ªï¼‰
        if api_key:
            global client  # å¼•ç”¨å¤–éƒ¨çš„ client å¯¹è±¡
            client = OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            )
        self.db_path = db_path
        self.paper_idx_path = paper_idx_path
        self.force_idx_path = force_idx_path
        self.dimension = 1536

        # 1. åˆå§‹åŒ– SQLite æ•°æ®åº“ (ç”¨äºå…ƒæ•°æ®æŒä¹…åŒ–å’ŒæŸ¥é‡)
        self._init_sqlite()

        # 2. åˆå§‹åŒ–æˆ–åŠ è½½ FAISS ç´¢å¼•
        if os.path.exists(self.paper_idx_path):
            index = faiss.read_index(self.paper_idx_path)
            if not isinstance(index, faiss.IndexIDMap):
                print("âš ï¸ ç´¢å¼•ä¸æ˜¯ IDMapï¼Œè‡ªåŠ¨åŒ…è£…...")
                index = faiss.IndexIDMap(index)
            self.paper_index = index

            # self.paper_index = faiss.read_index(self.paper_idx_path)
            print(f"ä»ç£ç›˜åŠ è½½è®ºæ–‡ç´¢å¼•ï¼Œå½“å‰è§„æ¨¡: {self.paper_index.ntotal}")
        else:
            # self.paper_index = faiss.IndexFlatL2(self.dimension)
            base_index = faiss.IndexFlatL2(self.dimension)
            self.paper_index = faiss.IndexIDMap(base_index)

        if os.path.exists(self.force_idx_path):
            index = faiss.read_index(self.force_idx_path)
            if not isinstance(index, faiss.IndexIDMap):
                print("âš ï¸ ç´¢å¼•ä¸æ˜¯ IDMapï¼Œè‡ªåŠ¨åŒ…è£…...")
                index = faiss.IndexIDMap(index)
            self.force_index = index

            # self.force_index = faiss.read_index(self.force_idx_path)
            print(f"ä»ç£ç›˜åŠ è½½åŠ›åœºç´¢å¼•ï¼Œå½“å‰è§„æ¨¡: {self.force_index.ntotal}")
        else:
            # self.force_index = faiss.IndexFlatL2(self.dimension)
            base_index = faiss.IndexFlatL2(self.dimension)
            self.force_index = faiss.IndexIDMap(base_index)

    def _init_sqlite(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("PRAGMA journal_mode=WAL;")
            conn.execute("PRAGMA synchronous=NORMAL;")

            cursor = conn.cursor()
            # è®ºæ–‡è¡¨ï¼šä»¥æ ‡é¢˜ä½œä¸ºå”¯ä¸€çº¦æŸè¿›è¡ŒæŸ¥é‡
            cursor.execute('''CREATE TABLE IF NOT EXISTS papers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT UNIQUE,
                metadata_json TEXT,
                vector_id INTEGER
            )''')
            # åŠ›åœºè¡¨ï¼šä»¥å…¬å¼å’ŒèƒŒæ™¯çš„ç»„åˆå“ˆå¸Œä½œä¸ºå”¯ä¸€çº¦æŸ
            cursor.execute('''CREATE TABLE IF NOT EXISTS force_fields (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                formula_hash TEXT UNIQUE,
                force_json TEXT,
                source_paper TEXT,
                vector_id INTEGER
            )''')
            # 3. æ–°å¢ï¼šå›¾è¡¨ä¿¡æ¯è¡¨
            # å­˜å‚¨æ¯å¼ å›¾ç‰‡çš„è·¯å¾„ã€æ ‡æ³¨ã€æ‰€å±é¡µç ï¼Œä»¥åŠé¢„ç•™çš„å‘é‡ ID
            cursor.execute('''CREATE TABLE IF NOT EXISTS figures (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            paper_id INTEGER,
                            image_path TEXT,
                            caption TEXT,
                            page_num INTEGER,
                            figure_vector_id INTEGER,
                            FOREIGN KEY (paper_id) REFERENCES papers (id)
                        )''')
            conn.commit()

    def extract_figures(self, file_path: str, structured_text: dict):
        """
        ä» PDF ä¸­æå–å›¾åƒï¼Œå¹¶å°è¯•åŸºäºè§†è§‰æ¨¡å‹ä¸ç‰©ç†å‚æ•°è¿›è¡Œè¯­ä¹‰å¯¹æ ‡ã€‚

        å½“å‰ç‰ˆæœ¬ï¼ˆStep 2 & 3 åŸå‹ï¼‰ï¼š
        - ä»æŒ‰â€œæ•´é¡µæˆªå›¾â€æ–¹å¼ä¸ºæ¯ä¸€é¡µç”Ÿæˆä¸€å¼  PNGï¼ˆåç»­å¯å‡çº§ä¸ºç²¾ç¡®å›¾è¡¨è£å‰ªï¼‰ã€‚
        - å¯¹æ¯ä¸€é¡µå›¾åƒï¼Œç»“åˆ structured_text['parameters']ï¼Œè°ƒç”¨è§†è§‰å¤§æ¨¡å‹ï¼ˆé¢„ç•™æ¥å£ï¼‰æ¨æ–­ï¼š
          * è¯¥å›¾çš„ç‰©ç†å«ä¹‰ captionï¼ˆä¾§é‡æè¿°ä¸æ¨¡æ‹Ÿå‚æ•°çš„å…³ç³»ï¼‰ï¼›
          * ä¸ä¹‹æœ€ç›¸å…³çš„ç‰©ç†å‚æ•°åˆ—è¡¨ linked_parametersã€‚
        - è¿”å›çš„æ•°æ®ç»“æ„ä¸ JSON ä¸­ figures å­—æ®µä¿æŒä¸€è‡´ï¼Œä¾¿äºå‰ç«¯å’Œæ•°æ®åº“ç›´æ¥ä½¿ç”¨ã€‚
        """
        figures = []
        try:
            pdf_path = Path(file_path)
            if not pdf_path.exists():
                return figures

            # ç‰©ç†å‚æ•°åˆ—è¡¨ï¼ˆç”¨äºå¯¹æ ‡ï¼‰
            params = structured_text.get("parameters", []) or []
            param_summaries = []
            for p in params:
                name = p.get("name", "")
                symbol = p.get("symbol", "")
                unit = p.get("unit", "")
                meaning = p.get("meaning", "")
                line = f"åç§°: {name}, ç¬¦å·: {symbol}, å•ä½: {unit}, å«ä¹‰: {meaning}"
                param_summaries.append(line)
            param_summary_text = "\n".join(param_summaries) if param_summaries else "ï¼ˆå½“å‰æœªæå–åˆ°ä»»ä½•å‚æ•°ï¼‰"

            # è¾“å‡ºç›®å½•ï¼š<é¡¹ç›®æ ¹>/figures/<pdf_stem>/
            base_dir = PROJECT_ROOT / "figures" / pdf_path.stem
            base_dir.mkdir(parents=True, exist_ok=True)
            print(f"[extract_figures] pdf_path={pdf_path}, base_dir={base_dir}")

            doc = fitz.open(str(pdf_path))
            max_pages_for_figures = 6  # é™åˆ¶å¤„ç†é¡µæ•°ï¼Œé¿å…è¿‡å¤šè°ƒç”¨ VLM

            for page_index in range(len(doc)):
                if page_index >= max_pages_for_figures:
                    break
                page = doc[page_index]
                # å…ˆä½¿ç”¨æ•´é¡µæˆªå›¾ä½œä¸ºç²—ç²’åº¦å›¾è¡¨ï¼ˆåç»­å¯æ¼”è¿›ä¸ºå›¾è¡¨åŒºåŸŸè£å‰ªï¼‰
                pix = page.get_pixmap(dpi=160)
                img_name = f"{pdf_path.stem}_p{page_index + 1}.png"
                img_path = base_dir / img_name
                pix.save(str(img_path))

                abs_img_path = img_path.resolve()
                try:
                    # å¯¹æ•°æ®åº“ä¸å‰ç«¯éƒ½å­˜å‚¨ä¸ºã€ç›¸å¯¹é¡¹ç›®æ ¹ç›®å½•ã€‘çš„è·¯å¾„
                    rel_img_path = abs_img_path.relative_to(PROJECT_ROOT)
                    image_path_str = rel_img_path.as_posix()
                except ValueError:
                    # ç†è®ºä¸Šä¸ä¼šèµ°åˆ°è¿™é‡Œï¼ˆbase_dir å·²ç»åœ¨ PROJECT_ROOT ä¸‹ï¼‰ï¼Œå…œåº•æ‰“å°
                    image_path_str = abs_img_path.as_posix()
                    print(f"[extract_figures] WARNING: image not under PROJECT_ROOT, stored abs path: {image_path_str}")

                print(f"[extract_figures] page={page_index + 1}, img_abs={abs_img_path}, stored_rel={image_path_str}")

                # é˜¶æ®µ B&Cï¼šè°ƒç”¨è§†è§‰æ¨¡å‹åšç‰©ç†è¯­ä¹‰å¯¹æ ‡ï¼ˆå¸¦å¼ºå…œåº•ï¼‰
                caption, linked_params = self._annotate_figure_with_vlm(
                    abs_img_path, page_index + 1, param_summary_text
                )

                figures.append({
                    "id": f"page-{page_index + 1}",
                    "caption": caption,
                    "page": page_index + 1,
                    "linked_parameters": linked_params,
                    "image_path": image_path_str
                })

            doc.close()
        except Exception as e:
            print(f"âš ï¸ extract_figures å‘ç”Ÿå¼‚å¸¸: {e}")

        return figures

    def _annotate_figure_with_vlm(self, abs_img_path: Path, page_index: int, param_summary_text: str):
        """
        ä½¿ç”¨è§†è§‰å¤§æ¨¡å‹ï¼ˆQwen-VL ç­‰ï¼‰ä¸ºå•é¡µå›¾åƒç”Ÿæˆç‰©ç†è¯­ä¹‰è¯´æ˜å’Œå‚æ•°å…³è”ã€‚

        ä¸ºäº†å…¼å®¹å½“å‰ç¯å¢ƒï¼š
        - å¦‚æœè°ƒç”¨å¤±è´¥æˆ–è¶…æ—¶ï¼Œä¼šå›é€€åˆ°ç®€å•çš„å ä½å¼ captionï¼Œlinked_parameters ä¸ºç©ºåˆ—è¡¨ã€‚
        - å…·ä½“çš„ VLM è°ƒç”¨ç»†èŠ‚å¯èƒ½éœ€æ ¹æ®å®é™… DashScope / OpenAI æ¥å£ç¨ä½œè°ƒæ•´ã€‚
        """
        default_caption = f"è‡ªåŠ¨å¯¼å‡ºçš„ç¬¬ {page_index} é¡µæ•´ä½“å¿«ç…§ï¼ˆå¾…è§†è§‰æ¨¡å‹ç»†åŒ–ä¸ºå…³é”®å›¾è¡¨ï¼‰"
        fallback = (default_caption, [])

        try:
            prompt = f"""
ä½ æ˜¯ä¸€åå¤æ‚ç­‰ç¦»å­ä½“ç‰©ç†å­¦å®¶å’Œå›¾åƒç†è§£åŠ©æ‰‹ã€‚ç°åœ¨ç»™ä½ ä¸€å¼ è®ºæ–‡ä¸­çš„å›¾åƒï¼ˆæ¥è‡ªç¬¬ {page_index} é¡µï¼‰ä»¥åŠè¿™ç¯‡è®ºæ–‡ä¸­å·²ç»æå–çš„ç‰©ç†å‚æ•°åˆ—è¡¨ã€‚

ã€ç‰©ç†å‚æ•°åˆ—è¡¨ã€‘ï¼ˆæ¯è¡Œä¸ºä¸€ä¸ªå‚æ•°ï¼‰ï¼š
{param_summary_text}

ã€ä»»åŠ¡ã€‘è¯·å®Œæˆä»¥ä¸‹ä¸¤ç‚¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼å›ç­”ï¼š
1. ç”¨ 1 å¥è¯ï¼ˆä¸è¶…è¿‡ 40 ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰è¯´æ˜è¿™å¼ å›¾ä¸»è¦å±•ç¤ºäº†ä»€ä¹ˆç‰©ç†ç°è±¡æˆ–å‚æ•°å…³ç³»ï¼Œç‰¹åˆ«æ˜¯ä¸å“ªäº›å‚æ•°æœ‰å…³ï¼ˆä¾‹å¦‚ï¼šå±•ç¤ºäº†éš M_T^2 å¢å¤§é“¾çŠ¶ç»“æ„å½¢æˆçš„æ¼”åŒ–è¶‹åŠ¿ï¼‰ã€‚
2. ä»å‚æ•°åˆ—è¡¨ä¸­æŒ‘å‡ºä¸è¯¥å›¾æœ€ç›¸å…³çš„ 1â€“3 ä¸ªå‚æ•°ï¼Œè¿”å›å®ƒä»¬çš„â€œç¬¦å· symbolâ€æˆ–â€œåç§° nameâ€ï¼ˆåŸæ ·è¿”å›å³å¯ï¼‰ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼ˆä¸¥æ ¼ JSONï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™è¯´æ˜ï¼‰ï¼š
{{
  "caption": "ä¸€å¥è¯ç‰©ç†è¯´æ˜",
  "linked_parameters": ["ç¬¦å·æˆ–åç§°1", "ç¬¦å·æˆ–åç§°2"]
}}
"""
            # ä½¿ç”¨ base64 data URI æ–¹å¼ç›´æ¥ä¼ è¾“å›¾åƒï¼Œé¿å…è¿œç¨‹æ— æ³•è®¿é—®æœ¬åœ°è·¯å¾„çš„é—®é¢˜
            try:
                with open(abs_img_path, "rb") as f:
                    b64_bytes = base64.b64encode(f.read())
                    b64_str = b64_bytes.decode("ascii")
                data_uri = f"data:image/png;base64,{b64_str}"
            except Exception as e:
                print(f"âŒ [VLM] æœ¬åœ°å›¾åƒè¯»å–/base64 ç¼–ç å¤±è´¥(page={page_index}): {repr(e)}")
                return fallback

            # æŒ‰ DashScope OpenAI å…¼å®¹å¤šæ¨¡æ€è§„èŒƒï¼šcontent ä¸ºè‹¥å¹²ä¸ª {type: \"text\"|\"image_url\", ...}
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt,
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": data_uri,
                            },
                        },
                    ],
                }
            ]

            print(f"[VLM] annotating figure page={page_index}, path={abs_img_path}, via=base64")
            try:
                vlm_response = client.chat.completions.create(
                    model="qwen-vl-max",  # æˆ–å…¶ä»–å¯ç”¨è§†è§‰æ¨¡å‹ï¼Œå¦‚ qwen-vl-plus
                    messages=messages,
                    temperature=0.1,
                )
            except Exception as e:
                print(f"âŒ [VLM] qwen-vl è°ƒç”¨å¤±è´¥(page={page_index}): {repr(e)}")
                return fallback

            raw_content = vlm_response.choices[0].message.content
            if isinstance(raw_content, list):
                # å…¼å®¹éƒ¨åˆ† SDK ä¼šè¿”å› content ä¸ºå¤šæ®µçš„æƒ…å†µ
                text_parts = [c.get("text", "") for c in raw_content if isinstance(c, dict)]
                raw_text = "\n".join(text_parts).strip()
            else:
                raw_text = str(raw_content).strip()

            # å‰¥ç¦»å¯èƒ½çš„ ```json åŒ…è£¹
            if "```json" in raw_text:
                raw_text = raw_text.split("```json", 1)[1].split("```", 1)[0].strip()
            elif "```" in raw_text:
                raw_text = raw_text.split("```", 1)[1].split("```", 1)[0].strip()

            try:
                parsed = json.loads(raw_text)
            except Exception as e:
                print(f"âš ï¸ [VLM] è§£æ JSON å¤±è´¥(page={page_index}): {repr(e)}; raw_text={raw_text}")
                return fallback

            caption = parsed.get("caption", "").strip() or default_caption
            linked = parsed.get("linked_parameters", []) or []

            # è§„èŒƒåŒ– linked_parameters å†…å®¹ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
            norm_linked = []
            for item in linked:
                if isinstance(item, str):
                    norm_linked.append(item.strip())
                elif isinstance(item, dict):
                    # å¦‚æœæ¨¡å‹è¿”å›äº†å¸¦å­—æ®µçš„å¯¹è±¡ï¼Œå°è¯•å– name æˆ– symbol
                    val = item.get("symbol") or item.get("name")
                    if val:
                        norm_linked.append(str(val).strip())

            print(f"[VLM] page={page_index}, caption={caption}, linked={norm_linked}")
            return caption, norm_linked

        except Exception as e:
            print(f"âš ï¸ [VLM] annotate å‘ç”Ÿå¼‚å¸¸(page={page_index}): {repr(e)}")
            return fallback

    def extract_paper_structure(self, file_path):
        """
        ç¬¬ä¸€æ­¥ï¼šåŒæ¨¡å‹æµæ°´çº¿æå–
        Stage 1: qwen-long è´Ÿè´£æ·±åº¦ç‰©ç†ç†è§£ (æ–‡æœ¬è¾“å‡º)
        Stage 2: qwen-turbo è´Ÿè´£ä¸¥æ ¼ JSON æ ¼å¼åŒ–
        """
        print(f"ğŸš€ [é˜¶æ®µ 1] æ­£åœ¨è°ƒç”¨ qwen-long è¿›è¡Œæ·±åº¦ç‰©ç†æå–: {file_path}")
        try:
            file_object = client.files.create(file=Path(file_path), purpose="file-extract")
        except Exception as e:
            print(f"âŒ [é˜¶æ®µ 1] file-extract å¤±è´¥: {repr(e)}")
            raise

        # --- Stage 1: qwen-long æç¤ºè¯ (ä¾§é‡ç‰©ç†ä¸ç†è§£) ---
        extraction_prompt = """
        ä½ æ˜¯ä¸€ä¸ªç‰©ç†å­¦ä¸“å®¶ã€‚è¯·é˜…è¯»è®ºæ–‡ï¼Œæå–æ ¸å¿ƒä¿¡æ¯å¹¶æŒ‰ä»¥ä¸‹ã€æ ‡ç­¾æ ¼å¼ã€‘è¾“å‡ºã€‚
        æ³¨æ„ï¼šä¸è¦è¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºMarkdownï¼Œç›´æ¥è¾“å‡ºæ ‡ç­¾å’Œå†…å®¹ã€‚
        æ¯ä¸€ä¸ªå‚é‡å’ŒåŠ›åœºè¯·æŒ‰åˆ—è¡¨å½¢å¼åˆ—å‡ºã€‚

        è¾“å‡ºè§„èŒƒï¼š
        [metadata.title]: æ ‡é¢˜
        [metadata.journal]: æœŸåˆŠ
        [metadata.year]: å¹´ä»½
        [metadata.innovation]: åˆ›æ–°ç‚¹
        [physics_context.environment]: å®éªŒç¯å¢ƒ
        [physics_context.detailed_background]: èƒŒæ™¯æè¿°
        [observed_phenomena]: è§‚å¯Ÿåˆ°çš„ç‰©ç†ç°è±¡
        [simulation_results_description]: æ¨¡æ‹Ÿç»“æœæè¿°
        [keywords]: å…³é”®è¯1, å…³é”®è¯2
        [experiment_setup]: å®éªŒè£…ç½®æè¿°

        [parameter]: 
        åç§°: | ç¬¦å·: | æ•°å€¼: | å•ä½: | å«ä¹‰: | å¯ŒåŒ–ç‰©ç†æ„ä¹‰: | æ¥æº: (åŸæ–‡/æ¨æ–­)

        [force_field]:
        åç§°: | å…¬å¼: | ç‰©ç†æœ¬è´¨: | æ¨¡æ‹Ÿè®¡ç®—å»ºè®®(å«å•ä½):

        [interparticle_interaction]: 
        æ³¨æ„ï¼šæ­¤å¤„åªæå–æè¿°ã€å¾®ç²’ä¸å¾®ç²’ä¹‹é—´ã€‘ç›¸äº’ä½œç”¨çš„å¯¹åŠ¿ï¼ˆPair Potentialï¼‰æˆ–åŠ›åœº,åŒæ—¶æ­¤åŠ›åœºå°†è¢«ç”¨äºæ¨¡æ‹Ÿ,æ‰€ä»¥åªèƒ½æœ‰ä¸€ä¸ªå¹¶ä¸”æœ€ä¸ºç¬¦åˆåŸæœ‰ç‰©ç†èƒŒæ™¯ã€‚
        - ä¸¥ç¦åŒ…å«ï¼šå¤–åŠ ç”µåœºï¼ˆExternal AC/DC Fieldï¼‰ã€é‡åŠ›ã€ç£åœºã€æ•´ä½“é™åˆ¶åŠ›ç­‰èƒŒæ™¯å‚æ•°ã€‚
        æ ¼å¼ï¼šåç§°: | å…¬å¼: | ç‰©ç†æœ¬è´¨: | æ¨¡æ‹Ÿè®¡ç®—å»ºè®®(å«å•ä½):
        """

        # ç¬¬ä¸€æ­¥è°ƒç”¨
        try:
            extraction_response = client.chat.completions.create(
                model="qwen-long",
                messages=[
                    {'role': 'system', 'content': f'fileid://{file_object.id}'},
                    {'role': 'system', 'content': extraction_prompt},
                    {'role': 'user', 'content': 'è¯·æŒ‰æ ¼å¼æå–è®ºæ–‡å†…å®¹ã€‚'}
                ],
                temperature=0.1,
            )
        except Exception as e:
            print(f"âŒ [é˜¶æ®µ 1] qwen-long è°ƒç”¨å¤±è´¥: {repr(e)}")
            # å‘ä¸ŠæŠ›å‡ºï¼Œè®©å‰ç«¯çœ‹åˆ°å…·ä½“é”™è¯¯ä¿¡æ¯ï¼ˆapp3 ä¼šå±•ç¤ºåœ¨â€œè§£æå¤±è´¥: ...â€é‡Œï¼‰
            raise

        try:
            extracted_text = extraction_response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ [é˜¶æ®µ 1] è§£æ qwen-long è¿”å›å†…å®¹å¤±è´¥: {repr(e)}; raw_response={extraction_response}")
            raise

        print("âœ… [é˜¶æ®µ 1] æå–å®Œæˆï¼Œè¿›å…¥é˜¶æ®µ 2 æ ¼å¼åŒ–...")

        # --- Stage 2: qwen-turbo æç¤ºè¯ (ä¾§é‡æ ¼å¼ä¸æ ¡éªŒ) ---
        formatting_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„JSONè½¬æ¢åŠ©æ‰‹ã€‚
        ä»»åŠ¡ï¼šå°†ç”¨æˆ·æä¾›çš„ã€æ ‡ç­¾åŒ–ç‰©ç†æ•°æ®ã€‘è½¬æ¢ä¸ºä¸¥æ ¼çš„JSONæ ¼å¼ã€‚

        è¦æ±‚ï¼š
        1. ä¸¥æ ¼éµå®ˆä»¥ä¸‹JSONç»“æ„ã€‚
        2. ç¡®ä¿å•ä½(unit)å’Œæ•°å€¼(value)åˆ†ç¦»ã€‚
        3. ç¦æ­¢è¾“å‡ºä»»ä½•è§£é‡Šæ–‡å­—ï¼Œåªè¾“å‡ºJSONä¸»ä½“ã€‚
        4. ç¦æ­¢ trailing commasã€‚
        5. ç‰©ç†å…¬å¼ä½¿ç”¨ Latex è¯­æ³•ã€‚
           - å¯¹äº force_fields[i].formula å­—æ®µï¼šåªå¡«å†™ã€å•ä¸ª LaTeX å…¬å¼æœ¬ä½“ã€‘ï¼Œä¸è¦åœ¨å­—ç¬¦ä¸²ä¸­å†åŒ…è£¹ $ æˆ– $$ï¼Œä¹Ÿä¸è¦åŠ å…¥é¢å¤–è¯´æ˜æ–‡å­—ã€‚
           - ç¤ºä¾‹ï¼šæ­£ç¡®: "W(r,\\theta) = \\frac{Q^2}{r} e^{-r/\\lambda}"ï¼›é”™è¯¯: "$ W(r,\\theta) = ... $" æˆ– "W(...) = ..., å…¶ä¸­ Q è¡¨ç¤ºç”µè·"ã€‚
        6. é’ˆå¯¹ force_fields å­—æ®µï¼šåªä¿ç•™ã€å¾®ç²’é—´ç›¸äº’ä½œç”¨åŠ¿ï¼ˆPair Potentialsï¼‰ã€‘ã€‚å¦‚æœè¾“å…¥ä¸­åŒ…å«â€œå¤–åŠ ç”µåœºâ€æˆ–â€œèƒŒæ™¯åœºâ€ï¼Œè¯·å°†å…¶ç‰©ç†å‚æ•°å½’ç±»åˆ° parameters ä¸­ï¼Œä¸¥ç¦æ”¾å…¥ force_fieldsã€‚

        ç›®æ ‡ç»“æ„ï¼ˆè¯·ä¸¥æ ¼éµå®ˆå­—æ®µåå’Œå±‚çº§ï¼›é™¤ formula å­—æ®µå¤–ï¼Œå…¶ä»–åŒ…å«æ•°å­¦ç¬¦å·çš„åœ°æ–¹å¯ä»¥ä½¿ç”¨å†…åµŒ $...$ ä»¥ä¾¿å‰ç«¯æ¸²æŸ“ï¼‰ï¼š
        {
            "metadata": {"title": "", "journal": "", "year": "", "innovation": ""},
            "physics_context": {"environment": "", "detailed_background": ""},
            "observed_phenomena": "",
            "simulation_results_description": "",
            "keywords": [],
            "parameters": [
                {
                    "name": "", "symbol": "", "value": "", "unit": "",
                    "meaning": "", "enriched_physics": "", "source": ""
                }
            ],
            "force_fields": [
                {
                    "name": "", "formula": "",
                    "physical_significance": "", "computational_hint": ""
                }
            ],
            "experiment_setup": "",
            "figures": [
                {
                    "id": "",                 
                    "caption": "",            
                    "page": 0,                
                    "linked_parameters": [],  
                    "image_path": ""         
                }
            ]
        }
        """

        # ç¬¬äºŒæ­¥è°ƒç”¨ï¼šä½¿ç”¨æ›´æ“…é•¿æ ¼å¼åŒ–çš„ qwen-turbo (æˆ– qwen-plus)
        try:
            format_response = client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {'role': 'system', 'content': formatting_prompt},
                    {'role': 'user', 'content': f"è¯·å°†ä»¥ä¸‹å†…å®¹è½¬æ¢ä¸ºJSONï¼š\n\n{extracted_text}"}
                ],
                temperature=0,  # æä½æ¸©åº¦ç¡®ä¿ç¨³å®šæ€§
            )
        except Exception as e:
            print(f"âŒ [é˜¶æ®µ 2] qwen-plus è°ƒç”¨å¤±è´¥: {repr(e)}")
            raise

        try:
            raw_json = format_response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ [é˜¶æ®µ 2] è§£æ qwen-plus è¿”å›å†…å®¹å¤±è´¥: {repr(e)}; raw_response={format_response}")
            raise

        # --- å®‰å…¨è§£æä¸å…œåº•é€»è¾‘ ---
        # æ¨¡æ¿å®šä¹‰ï¼ˆæ–°å¢ figures å­—æ®µï¼Œä¿è¯ä¸‹æ¸¸ç»“æ„ä¸€è‡´ï¼‰
        default_structure = {
            "metadata": {
                "title": f"è§£æå¤±è´¥_{Path(file_path).name}",
                "journal": "Unknown",
                "year": "Unknown",
                "innovation": "None"
            },
            "physics_context": {"environment": "Unknown", "detailed_background": "None"},
            "observed_phenomena": "None",
            "simulation_results_description": "None",
            "keywords": [],
            "parameters": [],
            "force_fields": [],
            "experiment_setup": "None",
            "figures": []
        }

        # å‰¥ç¦»ä»£ç å—
        if "```json" in raw_json:
            raw_json = raw_json.split("```json")[1].split("```")[0].strip()
        elif "```" in raw_json:
            raw_json = raw_json.split("```")[1].split("```")[0].strip()

        try:
            structured_data = json.loads(raw_json)
            # æ·±åº¦åˆå¹¶ç¡®ä¿ä¸ä¸¢å¤±Key
            import copy
            final_data = copy.deepcopy(default_structure)
            # ç®€å•çš„é€»è¾‘åˆå¹¶
            for k, v in structured_data.items():
                if isinstance(v, dict) and k in final_data and isinstance(final_data[k], dict):
                    final_data[k].update(v)
                else:
                    final_data[k] = v

            # è°ƒç”¨å›¾åƒæå–é’©å­ï¼Œè¡¥å…… figures å­—æ®µ
            try:
                figures = self.extract_figures(file_path, final_data)
                if figures:
                    final_data["figures"] = figures
            except Exception as e:
                print(f"âš ï¸ æå–å›¾åƒå¤±è´¥: {e}")

            print(final_data)
            return final_data

        except json.JSONDecodeError as e:
            print(f"âŒ JSON è½¬æ¢é˜¶æ®µå¤±è´¥: {e}")
            # ä¿åº•æ–¹æ¡ˆï¼šä» extracted_text ä¸­æ­£åˆ™æŠ¢æ•‘ä¸€ä¸ªæ ‡é¢˜
            import re
            title_match = re.search(r'\[metadata\.title\]:\s*(.*)', extracted_text)
            if title_match:
                default_structure["metadata"]["title"] = title_match.group(1).strip()
            return default_structure

    def search_knowledge(self, query_text, top_k=2):
        """åŸºäºå‘é‡æ£€ç´¢ç»“æœï¼Œä» SQLite ç¡¬ç›˜æ•°æ®åº“å›æè¯¦å°½å…ƒæ•°æ®"""
        query_vector = self.get_embedding(query_text)

        D1, I1 = self.paper_index.search(query_vector, top_k)
        D2, I2 = self.force_index.search(query_vector, top_k)

        relevant_papers = []
        relevant_forces = []

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ä» I1 (è®ºæ–‡å‘é‡ ID åˆ—è¡¨) å›æ
            for v_id in I1[0]:
                if v_id == -1: continue
                cursor.execute("SELECT metadata_json FROM papers WHERE vector_id = ?", (int(v_id),))
                res = cursor.fetchone()
                if res: relevant_papers.append(json.loads(res[0]))

            # ä» I2 (åŠ›åœºå‘é‡ ID åˆ—è¡¨) å›æ
            for v_id in I2[0]:
                if v_id == -1: continue
                cursor.execute("SELECT force_json, source_paper FROM force_fields WHERE vector_id = ?", (int(v_id),))
                res = cursor.fetchone()
                if res:
                    f_data = json.loads(res[0])
                    f_data['source_from'] = res[1]  # é™„å¸¦æ¥æºä¿¡æ¯
                    relevant_forces.append(f_data)

        return relevant_papers, relevant_forces

    def get_simulation_recommendation(self, structured_paper, user_params):
        """
        ç¬¬äºŒæ­¥ï¼šåŠ¨æ€é’ˆå¯¹ç”¨æˆ·æä¾›çš„æ¯ä¸ªå‚æ•°ï¼ˆåŠå…¶ç‰©ç†å«ä¹‰ï¼‰ç»™å‡ºæ¨è
        """
        # RAG æ£€ç´¢é€»è¾‘
        # å¢åŠ å®‰å…¨è·å–é€»è¾‘ï¼Œé˜²æ­¢ keywords ä¸­æ··å…¥ int / float å¯¼è‡´ join æŠ¥é”™
        title = structured_paper.get('metadata', {}).get('title', 'Unknown')
        raw_keywords = structured_paper.get('keywords', [])
        if isinstance(raw_keywords, (list, tuple, set)):
            keywords = [str(k) for k in raw_keywords]
        elif raw_keywords:
            keywords = [str(raw_keywords)]
        else:
            keywords = []
        search_query = f"{title} " + " ".join(keywords)
        relevant_papers, relevant_forces = self.search_knowledge(search_query)

        # ä¿®æ”¹ç‚¹ï¼šæå–å‚æ•°ååˆ—è¡¨ï¼ˆæ­¤æ—¶ user_params æ˜¯åµŒå¥—å­—å…¸ï¼‰
        # è·å–ç”¨æˆ·æœŸæœ›ç°è±¡
        expected_phenomena = user_params.get("expected_phenomena", "æ— ")
        param_list_str = ", ".join([k for k in user_params.keys() if k != "expected_phenomena"])

        # ä¿®æ”¹ç‚¹ï¼šPrompt å¼ºåŒ–äº†å¯¹ç‰©ç†å«ä¹‰ï¼ˆdescriptionï¼‰çš„å…³æ³¨
        prompt = f"""
        ä½ ç°åœ¨æ˜¯ä¸€åå¤æ‚ç­‰ç¦»å­ä½“ç‰©ç†æ¨¡æ‹Ÿä¸“å®¶ã€‚

        ã€ç‰©ç†å‚è€ƒä¸Šä¸‹æ–‡ã€‘ï¼š
        - å‚è€ƒæ–‡çŒ®æ ¸å¿ƒç»“æ„ï¼š{json.dumps(structured_paper, indent=2)}
        - å…³è”åŠ›åœºåº“çŸ¥è¯†ï¼š{json.dumps(relevant_forces, indent=2)}

        ã€å‚è€ƒè®ºæ–‡ç‰©ç†ç°è±¡ã€‘ï¼š
        - å®éªŒè§‚å¯Ÿï¼š{structured_paper.get('observed_phenomena')}
        - æ¨¡æ‹Ÿè¡¨ç°ï¼š{structured_paper.get('simulation_results_description')}
        - å‚è€ƒåŠ›åœºï¼š{json.dumps(relevant_forces, indent=2)}

        ã€ç”¨æˆ·æ¨¡æ‹Ÿéœ€æ±‚ã€‘ï¼š
        - å¾…æ¨¡æ‹Ÿå‚æ•°ï¼š{json.dumps({k: v for k, v in user_params.items() if k != 'expected_phenomena'}, indent=2)}
        - æœŸæœ›è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼š{expected_phenomena}

        ã€ä»»åŠ¡æŒ‡ä»¤ã€‘ï¼š
        è¯·é’ˆå¯¹ä¸Šè¿°æ¯ä¸ªå‚æ•°ï¼Œç»“åˆå…¶ã€ç‰©ç†å«ä¹‰è¯´æ˜ã€‘å’Œå‚è€ƒæ–‡çŒ®ä¸­çš„å®éªŒ/ç†è®ºèƒŒæ™¯ï¼Œè¿›è¡Œç²¾ç¡®çš„åŒºé—´æ¨èï¼š
        ç°è±¡åŒ¹é…ï¼šæ ¹æ®ç”¨æˆ·æœŸæœ›è§‚å¯Ÿåˆ°çš„ã€{expected_phenomena}ã€‘ï¼Œç»“åˆå‚è€ƒè®ºæ–‡ä¸­çš„ç°è±¡æè¿°ï¼Œè°ƒæ•´æ¨èçš„å‚æ•°åŒºé—´ã€‚
        1. æ¨èåŒºé—´ [Min, Max]ï¼šå¿…é¡»ç¬¦åˆç‰©ç†æè¿°ä¸­çš„æ—¶ç©ºå°ºåº¦è¦æ±‚ã€‚
        2. å»ºè®®æ­¥é•¿ (Step Size)ï¼šå¿…é¡»è¶³ä»¥æ•æ‰ç‰©ç†æè¿°ä¸­æåˆ°çš„å…³é”®ç‰¹å¾ï¼ˆå¦‚æ³¢å½¢è§£æã€ç¢°æ’é¢‘ç‡ç­‰ï¼‰ã€‚
        3. æ·±åº¦ç†ç”±ï¼šè¯·å¼•ç”¨å‚è€ƒæ–‡çŒ®ä¸­çš„å…¬å¼æˆ–ç‰©ç†å¸¸æ•°ï¼ˆå¦‚ç­‰ç¦»å­ä½“é¢‘ç‡ Ï‰pdï¼‰æ¥æ”¯æ’‘ä½ çš„åŒºé—´é€‰æ‹©ã€‚
        4. å•ä½æ ¸å¿ƒåŸåˆ™ï¼šæ‰€æœ‰æ¨èçš„åŒºé—´ [Min, Max] å’Œæ­¥é•¿ val å¿…é¡»ä¸¥æ ¼åŒ¹é…ç‰©ç†å•ä½ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„æ˜¯ 'ms'ï¼Œæ¨èä¹Ÿå¿…é¡»ä»¥ 'ms' ä¸ºåŸºå‡†ï¼Œä¸¥ç¦å•ä½æ··ä¹±ã€‚  

        æ­¤å¤–ï¼Œè¯·æ ¹æ®ç‰©ç†å«ä¹‰æ¨èä¸€ä¸ªæœ€åŒ¹é…çš„ã€æ¨¡æ‹ŸåŠ›åœºæ¨¡å‹ã€‘ã€‚

        ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘ï¼ˆä¸¥æ ¼ JSONï¼‰ï¼š
        {{
          "parameter_recommendations": {{
             "å‚æ•°å": {{
                 "range": [min, max], 
                 "step": val, 
                 "unit": "ç‰©ç†å•ä½", 
                 "reason": "ç»“åˆç°è±¡åŒ¹é…å’Œå•ä½çº¦æŸçš„ç†ç”±"
             }}
          }},
          "force_field_recommendation": {{
             "name": "åŠ›åœºåç§°",
             "reason": "ä¸ºä»€ä¹ˆè¯¥åŠ›åœºèƒ½å¤ç°æœŸæœ›ç°è±¡çš„ç‰©ç†ä¾æ®"
          }}
        }}
        """

        completion = client.chat.completions.create(
            model="qwen-long",
            messages=[
                {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªç²¾é€šå¤æ‚ç­‰ç¦»å­ä½“ç‰©ç†å’Œæ•°å€¼ç®—æ³•çš„èµ„æ·±ç§‘å­¦å®¶ã€‚è¯·ç›´æ¥è¾“å‡ºJSONã€‚'},
                {'role': 'user', 'content': prompt}
            ]
        )

        return completion.choices[0].message.content

    def get_embedding(self, text):
        """è°ƒç”¨é˜¿é‡Œäº‘é…å¥— Embedding æ¨¡å‹"""
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç®€å•å¤„ç†ï¼Œç¡®ä¿å®ƒæ˜¯å­—ç¬¦ä¸²ä¸”ä¸ä¸ºç©º
        if not text or not text.strip():
            text = "empty_input_placeholder"  # é˜²æ­¢ç©ºå­—ç¬¦ä¸²å¯¼è‡´APIæŠ¥é”™
        text = text.replace("\n", " ")

        response = client.embeddings.create(
            model="text-embedding-v2",  # é…å¥—çš„å‘é‡æ¨¡å‹
            input=text
        )
        # å°†ç»“æœè½¬ä¸º float32 çš„ numpy æ•°ç»„ï¼Œå¹¶å¢åŠ ä¸€ä¸ªç»´åº¦ (1, 1536) ä»¥é€‚é… FAISS
        return np.array(response.data[0].embedding).astype('float32').reshape(1, -1)

    def _is_valid_physics_data(self, data):
        """
        è´¨é‡æ ¡éªŒé€»è¾‘ï¼šåˆ¤æ–­æå–çš„æ•°æ®æ˜¯å¦å…·å¤‡ç‰©ç†ç ”ç©¶ä»·å€¼
        """
        # 1. æ ‡é¢˜æ ¡éªŒ
        title = data.get('metadata', {}).get('title', "")
        if not title or "è§£æå¤±è´¥" in title or title == "Unknown":
            return False

        # 2. æ ¸å¿ƒç‰©ç†å†…å®¹æ ¡éªŒ
        # å¦‚æœ parameters å’Œ force_fields åŒæ—¶ä¸ºç©ºï¼Œè¯´æ˜æ²¡æå–åˆ°ä»»ä½•å…³é”®ç‰©ç†å»ºæ¨¡ä¿¡æ¯
        if not data: return False
        params = data.get('parameters', [])
        forces = data.get('force_fields', [])
        if not params and not forces:
            print("âš ï¸ è´¨é‡æ ¡éªŒå¤±è´¥ï¼šæœªæå–åˆ°ä»»ä½•ç‰©ç†å‚æ•°æˆ–åŠ›åœºä¿¡æ¯ã€‚")
            return False

        # 3. æ–‡æœ¬å¯ŒåŒ–ç¨‹åº¦æ ¡éªŒ
        # å¦‚æœèƒŒæ™¯æè¿°å’Œåˆ›æ–°ç‚¹éƒ½æ˜¯é»˜è®¤çš„ "None" æˆ– "Unknown"ï¼Œè¯´æ˜ç†è§£å¤±è´¥
        innovation = data.get('metadata', {}).get('innovation', "None")
        background = data.get('physics_context', {}).get('detailed_background', "None")
        if innovation in ["None", "Unknown"] and background in ["None", "Unknown"]:
            print("âš ï¸ è´¨é‡æ ¡éªŒå¤±è´¥ï¼šç‰©ç†èƒŒæ™¯ç†è§£ä¸ºç©ºã€‚")
            return False

        return True

    def _safe_save_index(self, index, path):
        """åŸå­åŒ–ä¿å­˜ FAISS ç´¢å¼•"""
        tmp_path = path + ".tmp"
        try:
            faiss.write_index(index, tmp_path)
            # os.replace æ˜¯åŸå­çš„ï¼Œç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§
            os.replace(tmp_path, path)
        except Exception as e:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
            raise IOError(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")

    def update_vector_db(self, structured_data):
        """
        æŒä¹…åŒ–æ›´æ–°ï¼šæŸ¥é‡ -> å†™å…¥SQLite -> å†™å…¥FAISS -> åŒæ­¥åˆ°ç£ç›˜æ–‡ä»¶
        """
        title = structured_data['metadata']['title']

        # 1. æ·±åº¦è´¨é‡æ ¡éªŒ
        if not self._is_valid_physics_data(structured_data):
            print(f"âŒ æ•°æ®è´¨é‡æœªè¾¾æ ‡ï¼Œä¸è®°å½•åˆ°æ•°æ®åº“ã€‚è¯·æ£€æŸ¥è®ºæ–‡æ ¼å¼æˆ– API é…é¢ã€‚")
            return

        # ä½¿ç”¨ portalocker ç»™ç´¢å¼•æ–‡ä»¶åŠ é”ï¼Œé˜²æ­¢å¤šè¿›ç¨‹åŒæ—¶å†™å…¥å¯¼è‡´æ–‡ä»¶æŸå
        # æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª .lock æ–‡ä»¶ä½œä¸ºä¿¡å·ç¯
        lock_path = self.db_path + ".lock"

        try:
            with portalocker.Lock(lock_path, timeout=10):
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    # å¼€å¯æ˜¾å¼äº‹åŠ¡ï¼šç¡®ä¿æ•°æ®åº“å’Œç´¢å¼•æ–‡ä»¶è¦ä¹ˆéƒ½æˆåŠŸï¼Œè¦ä¹ˆéƒ½å¤±è´¥
                    conn.execute("BEGIN")

                    # --- 1. è®ºæ–‡æŸ¥é‡ä¸å†™å…¥ ---
                    cursor.execute("SELECT id FROM papers WHERE title = ?", (title,))
                    if cursor.fetchone():
                        print(f"è·³è¿‡å·²å­˜åœ¨è®ºæ–‡: {title}")
                        return
                    else:
                        # paper_text = f"Title: {title}. Context: {structured_data['physics_context']['detailed_background']}"
                        background = structured_data.get('physics_context', {}).get('detailed_background',
                                                                                    'No background available')
                        paper_text = f"Title: {title}. Context: {background}"
                        paper_vec = self.get_embedding(paper_text)

                        # å†™å…¥ FAISS å†…å­˜å¹¶è·å–å½“å‰çš„ç´¢å¼•ä½ç½®
                        # vector_id = self.paper_index.ntotal
                        # self.paper_index.add(paper_vec)

                        # å†™å…¥ SQLite å’Œç£ç›˜æ–‡ä»¶
                        # cursor.execute("INSERT INTO papers (title, metadata_json, vector_id) VALUES (?, ?, ?)",
                        #                (title, json.dumps(structured_data), vector_id))

                        # 1. å…ˆå­˜æ•°æ®åº“ï¼Œæ‹¿åˆ°è‡ªå¢ ID
                        # ä½¿ç”¨è‡ªå¢çš„æ˜¾å¼ idï¼ˆå¦‚ä» SQLite è·å¾—çš„ paper row idï¼Œæˆ–ç”¨ time-based int idï¼‰
                        cursor.execute("INSERT INTO papers (title, metadata_json, vector_id) VALUES (?, ?, ?)",
                                       (title, json.dumps(structured_data), -1))
                        paper_row_id = cursor.lastrowid

                        # 2. æŠŠè¿™ä¸ª ID åŒæ­¥ç»™ FAISS
                        # ç”¨ paper_row_id ä½œä¸ºå¤–éƒ¨å‘é‡ id
                        self.paper_index.add_with_ids(paper_vec, np.array([paper_row_id], dtype='int64'))

                        # 3. æ›´æ–°æ•°æ®åº“ä¸­çš„ vector_id å­—æ®µ
                        # ç„¶åæ›´æ–° DB çš„ vector_id å­—æ®µä¸º paper_row_id
                        cursor.execute("UPDATE papers SET vector_id = ? WHERE id = ?",
                                       (int(paper_row_id), paper_row_id))

                        # 4. ç«‹å³å†™å›ç£ç›˜
                        # faiss.write_index(self.paper_index, self.paper_idx_path)
                        self._safe_save_index(self.paper_index, self.paper_idx_path)
                        print(f"âœ… è®ºæ–‡å·²å­˜å…¥: {title} (ID: {paper_row_id})")

                    # --- 2. æ–°å¢ï¼šå›¾ç‰‡ä¿¡æ¯å…¥åº“ ---
                    if "figures" in structured_data:
                        for fig in structured_data["figures"]:
                            print(
                                f"[update_vector_db] storing figure for paper={title}, image_path={fig.get('image_path')}")
                            cursor.execute('''
                                INSERT INTO figures (paper_id, image_path, caption, page_num)
                                VALUES (?, ?, ?, ?)
                            ''', (
                                paper_row_id,
                                fig.get("image_path"),
                                fig.get("caption"),
                                fig.get("page")
                            ))
                        print(f"âœ… å›¾ç‰‡å·²å­˜å…¥: {title} (ID: {paper_row_id})")

                    # --- 2. åŠ›åœºæŸ¥é‡ä¸å†™å…¥ ---
                    if "force_fields" in structured_data:
                        for ff in structured_data["force_fields"]:
                            # ç”Ÿæˆå”¯ä¸€ç‰¹å¾å“ˆå¸Œï¼šå…¬å¼ + ç‰©ç†ç¯å¢ƒ
                            # formula_str = ff['formula'] + structured_data['physics_context']['environment']
                            # ä¿®æ”¹åï¼ˆæ›´å®‰å…¨ï¼‰
                            formula_val = ff.get('formula', 'unknown_formula')
                            env_val = structured_data.get('physics_context', {}).get('environment', 'unknown_env')
                            formula_str = formula_val + env_val

                            f_hash = hashlib.md5(formula_str.encode()).hexdigest()

                            cursor.execute("SELECT id FROM force_fields WHERE formula_hash = ?", (f_hash,))
                            if cursor.fetchone():
                                continue  # ç›¸ä¼¼èƒŒæ™¯ä¸‹çš„ç›¸åŒå…¬å¼ï¼Œè·³è¿‡

                            force_feature = f"Interparticle Interaction: {ff['name']}. Significance: {ff['physical_significance']}"
                            force_vec = self.get_embedding(force_feature)

                            # f_vector_id = self.force_index.ntotal
                            # self.force_index.add(force_vec)
                            #
                            # cursor.execute(
                            #     "INSERT INTO force_fields (formula_hash, force_json, source_paper, vector_id) VALUES (?, ?, ?, ?)",
                            #     (f_hash, json.dumps(ff), title, f_vector_id))

                            # 1. å…ˆå­˜æ•°æ®åº“æ‹¿åˆ° ID
                            cursor.execute(
                                "INSERT INTO force_fields (formula_hash, force_json, source_paper, vector_id) VALUES (?, ?, ?, ?)",
                                (f_hash, json.dumps(ff), title, -1))
                            db_force_id = cursor.lastrowid

                            # 2. åŒæ­¥ ID åˆ°åŠ›åœºç´¢å¼•
                            self.force_index.add_with_ids(force_vec, np.array([db_force_id], dtype='int64'))

                            # 3. å›å¡« vector_id
                            cursor.execute("UPDATE force_fields SET vector_id = ? WHERE id = ?",
                                           (db_force_id, db_force_id))

                        # faiss.write_index(self.force_index, self.force_idx_path)
                        self._safe_save_index(self.force_index, self.force_idx_path)

                    conn.commit()

        except portalocker.exceptions.LockException:
            print("âŒ æ— æ³•è·å–æ–‡ä»¶é”ï¼Œå¯èƒ½æœ‰å…¶ä»–è¿›ç¨‹æ­£åœ¨å†™å…¥ã€‚")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“æ›´æ–°å‘ç”Ÿé”™è¯¯: {e}")
            # è¿™é‡Œè‡ªåŠ¨è§¦å‘ rollback (å› ä¸ºåœ¨ with sqlite3.connect å—å†…)


if __name__ == "__main__":
    rag_system = ComplexPlasmaRAG()

    # ç”¨æˆ·è¾“å…¥ï¼šå¢åŠ æœŸæœ›ç°è±¡ï¼Œå¹¶å¼ºåŒ–å•ä½æ„è¯†
    user_input_params = {
        "target_particle_charge": {
            "value": "1.2 * 10^4",
            "unit": "e",
            "description": "å•ä¸ªå°˜åŸƒå¾®ç²’æºå¸¦çš„ç”µè·é‡ã€‚"
        },
        "time_scale": {
            "value": "200.0",
            "unit": "ms",
            "description": "æ¨¡æ‹Ÿæ¼”åŒ–çš„æ€»æ—¶é•¿ã€‚"
        },
        "debye_length_target": {
            "value": "0.6",
            "unit": "mm",
            "description": "ç³»ç»Ÿé¢„æœŸçš„å¾·æ‹œå±è”½é•¿åº¦ã€‚"
        },
        "expected_phenomena": "è§‚å¯Ÿåˆ°å¾®ç²’åœ¨å¾®é‡åŠ›æµåœºä¸­ç”±äºéå¯¹ç§°ç›¸äº’ä½œç”¨å½¢æˆçš„é“¾çŠ¶ç»“æ„(string formation)ã€‚"
    }

    # æ‰§è¡Œæµç¨‹
    pdf_path = r"/getdata/output/Complex_Plasma_Simulation/pdfs/MDâ€”Ivlev_PRL_2008.pdf"
    structured_json = rag_system.extract_paper_structure(pdf_path)

    # æŒä¹…åŒ–æ›´æ–°ï¼ˆæ­¤æ—¶å†…éƒ¨å·²åŒ…å«ç°è±¡æè¿°å’Œæ¨æ–­æ ‡æ³¨ï¼‰
    rag_system.update_vector_db(structured_json)

    # è·å–åŒ…å«ç°è±¡åŒ¹é…å»ºè®®çš„æ¨è
    recommendation = rag_system.get_simulation_recommendation(structured_json, user_input_params)

    print("\n--- æ¨¡æ‹Ÿå‚æ•°æ¨èç»“æœ (ç°è±¡å¯¹æ ‡ç‰ˆ) ---")
    print(recommendation)